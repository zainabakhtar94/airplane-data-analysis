# Load the parquet file
on_time_dataframe = spark.read.parquet('data/on_time_performance.parquet')
on_time_dataframe.registerTempTable("on_time_performance")

# Dump the unneeded fields and filter nulls
tail_numbers = on_time_dataframe.rdd.map(lambda x: x.TailNum)
tail_numbers = tail_numbers.filter(lambda x: x != '')

# distinct() gets us unique tail numbers
unique_tail_numbers = tail_numbers.distinct()

# Store as JSON objects via a dataframe. Repartition to 1 to get 1 json file.
unique_records = unique_tail_numbers.map(lambda x: {'TailNum': x}).toDF()
unique_records.repartition(1).write.mode("overwrite").json("data/tail_numbers.json")

# Now from bash: ls data/tail_numbers.json/part*
